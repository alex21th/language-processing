{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from types import SimpleNamespace\nfrom collections import Counter\nimport os\nimport re\nimport pathlib\nimport subprocess\nimport array\nimport pickle\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATASET_VERSION = 'ca-100'\nDATASET_ROOT = f'../input/viquipdia/{DATASET_VERSION}'\nWORKING_ROOT = f'data/{DATASET_VERSION}'\nDATASET_PREFIX = 'ca.wiki'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = SimpleNamespace(\n    window_size = 5,\n    cutoff = 3,\n    maxtokens = 100000,\n    dataset = f'{DATASET_ROOT}/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}/{DATASET_PREFIX}',\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we are running this notebook in Google Colab we can use Google Drive as permanent storage of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only for Google Colab\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    pathlib.Path('/content/drive/My Drive/POE/vectors').mkdir(parents=True, exist_ok=True)\n    os.chdir('/content/drive/My Drive/POE/vectors')\nexcept:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Download the catalan wiki databases to our computer or Google Drive"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only the first time that we run the notebook outside Kaggle\nif not os.path.isfile(f'{DATASET_ROOT}/{DATASET_PREFIX}.train.tokens'):\n    pathlib.Path(DATASET_ROOT).mkdir(parents=True, exist_ok=True)\n    subprocess.call(['wget', f'https://github.com/jarfo/slt/releases/download/{DATASET_VERSION}/{DATASET_PREFIX}.test.tokens',  '-O', f'{DATASET_ROOT}/{DATASET_PREFIX}.test.tokens'])\n    subprocess.call(['wget', f'https://github.com/jarfo/slt/releases/download/{DATASET_VERSION}/{DATASET_PREFIX}.valid.tokens', '-O', f'{DATASET_ROOT}/{DATASET_PREFIX}.valid.tokens'])\n    subprocess.call(['wget', f'https://github.com/jarfo/slt/releases/download/{DATASET_VERSION}/{DATASET_PREFIX}.train.tokens', '-O', f'{DATASET_ROOT}/{DATASET_PREFIX}.train.tokens'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocabulary(object):\n    def __init__(self, pad_token='<pad>', unk_token='<unk>', eos_token='<eos>'):\n        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token\n        if pad_token is not None:\n            self.pad_index = self.add_token(pad_token)\n        if unk_token is not None:\n            self.unk_index = self.add_token(unk_token)\n        if eos_token is not None:\n            self.eos_index = self.add_token(eos_token)\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def get_index(self, token):\n        if isinstance(token, str):\n            return self.token2idx.get(token, self.unk_index)\n        else:\n            return [self.token2idx.get(t, self.unk_index) for t in token]\n\n    def __len__(self):\n        return len(self.idx2token)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, filename):\n        with open(filename, 'rb') as f:\n            self.__dict__.update(pickle.load(f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Punctuation:\n    html = re.compile(r'&apos;|&quot;')\n    punctuation = re.compile(r'[^\\w\\s·]|_')\n    spaces = re.compile(r'\\s+')\n    ela_geminada = re.compile(r'l · l')\n\n    def strip(self, s):\n        '''\n        Remove all punctuation characters.\n        '''\n        s = self.html.sub(' ', s)\n        s = self.punctuation.sub(' ', s)\n        s = self.spaces.sub(' ', s).strip()\n        s = self.ela_geminada.sub('l·l', s)\n        return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(input_path, output_path):\n    punc = Punctuation()\n    with open(input_path, 'r', encoding='utf-8') as inpf, open(output_path, 'w', encoding='utf-8') as outf:\n        for line in inpf:\n            line = punc.strip(line)\n            print(line, file=outf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_counter(file_path):\n    counter = Counter()\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                tokens = line.split()\n                counter.update(tokens)\n    return counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_vocabulary(token_counter, cutoff=3, maxtokens=None, verbose=1, eos_token=None):\n    vocab = Vocabulary(eos_token=eos_token)\n    total_count = sum(token_counter.values())\n    in_vocab_count = 0\n\n    for token, count in token_counter.most_common(maxtokens):\n        if count >= cutoff:\n            vocab.add_token(token)\n            in_vocab_count += count\n\n    if verbose:\n        OOV_count = total_count - in_vocab_count\n        print('OOV ratio: %.2f%%.' % (100*OOV_count / total_count))\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_index(file_path, vocab, eos_token=None):\n    index_list = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                if eos_token is not None:\n                    line += ' ' + eos_token\n                tokens = line.strip().split()\n                index_list.append([vocab.get_index(token) for token in tokens])\n    return index_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(idx_list, window_size, pad_index=0):\n    input = []\n    target = array.array('I')\n    left_window = window_size // 2\n    right_window = window_size - left_window - 1\n    for line in idx_list:\n        if len(line) <= window_size // 2:\n            continue\n        ext_line = [pad_index] * left_window + line + [pad_index] * right_window\n        for i, token_id in enumerate(line):\n            context = array.array('I', ext_line[i:i + left_window] + ext_line[i + left_window + 1:i + window_size])\n            input.append(context)\n            target.append(token_id)\n    return np.array(input, dtype=np.int32), np.array(target, dtype=np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_dataset(params):\n    dataset_prefix = params.dataset\n    working_prefix = params.working\n    cutoff = params.cutoff\n    maxtokens = params.maxtokens\n    window_size = params.window_size\n\n    data = []\n    for part in ['train', 'valid', 'test']:\n        data_filename = f'{dataset_prefix}.{part}.tokens'\n        data_filename_nopunct = f'{working_prefix}.{part}.tokens.nopunct'\n        remove_punctuation(data_filename, data_filename_nopunct)\n\n        if part == 'train':\n            # Basic token statistics\n            token_counter = get_token_counter(data_filename_nopunct)\n            print(f'Number of Tokens: {sum(token_counter.values())}')\n            print(f'Number of different Tokens: {len(token_counter)}')\n            pickle.dump(token_counter, open(f'{data_filename_nopunct}.dic', 'wb'))\n\n            # Token vocabulary\n            token_vocab = get_token_vocabulary(token_counter, cutoff=cutoff, maxtokens=maxtokens)\n            token_vocab.save(f'{working_prefix}.vocab')\n            print(f'Vocabulary size: {len(token_vocab)}')\n\n        # Token indexes\n        train_idx = get_token_index(data_filename_nopunct, token_vocab)\n        print(f'Number of lines ({part}): {len(train_idx)}')\n\n        # Get input and target arrays\n        idata, target = get_data(train_idx, window_size)\n        data.append((idata, target))\n        print(f'Number of samples ({part}): {len(target)}')\n\n        # Save numpy arrays\n        np.savez(f'{working_prefix}.{part}.npz', idata=idata, target=target)\n    return token_vocab, data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create working dir\npathlib.Path(WORKING_ROOT).mkdir(parents=True, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab, data = prepare_dataset(params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the vocabulary with some words with specific Catalan characters as 'ï' and 'l·l'"},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in ['raïm', 'intel·ligent']:\n    print(f'{word} -> {vocab.get_index(word)}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}